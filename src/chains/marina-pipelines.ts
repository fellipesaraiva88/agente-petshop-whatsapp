import { RunnableSequence, RunnablePassthrough } from '@langchain/core/runnables';
import { ChatOpenAI } from '@langchain/openai';
import { ChatPromptTemplate } from '@langchain/core/prompts';
import { StringOutputParser } from '@langchain/core/output_parsers';
import { StyleAwareMemory } from '../memory/StyleAwareMemory';
import { getMarinaMode } from '../prompts/marina-modes';
import { PersonalityArchetype } from '../services/PersonalityProfiler';
import { RetrievalChain } from '../rag/RetrievalChain';

/**
 * ü¶ú MARINA PIPELINES - LCEL (LangChain Expression Language)
 *
 * 4 PIPELINES OTIMIZADOS:
 * 1. SIMPLES - Sauda√ß√µes, mensagens curtas (1-2s resposta)
 * 2. CONVERS√ÉO - Cliente quente, foco em fechamento (2-3s)
 * 3. VIP - Cliente premium, tratamento especial (2-3s)
 * 4. COMPLETO - An√°lise comportamental full (3-5s)
 *
 * VANTAGENS LCEL:
 * - Composi√ß√£o declarativa (n√£o imperativa)
 * - Parallel execution autom√°tico
 * - Retry/fallback integrado
 * - Streaming support
 * - Callbacks autom√°ticos
 */

interface PipelineInput {
  message: string;
  chatId: string;
  userName?: string;
  petName?: string;
  archetype?: PersonalityArchetype;
  sentiment?: string;
  urgency?: string;
  conversionScore?: number;
  isVip?: boolean;
  isNewClient?: boolean;
  fullContext?: string;
}

interface PipelineOutput {
  response: string;
  metadata: {
    pipelineUsed: string;
    processingTime?: number;
    characterCount: number;
  };
}

/**
 * üü¢ PIPELINE 1: SIMPLES
 * Para: Sauda√ß√µes ("oi", "ol√°"), mensagens curtas
 * Objetivo: Resposta R√ÅPIDA (1-2s)
 */
export function createSimplePipeline(
  openaiApiKey: string,
  memory: StyleAwareMemory
) {
  const llm = new ChatOpenAI({
    openAIApiKey: openaiApiKey,
    modelName: 'gpt-4o-mini',
    temperature: 0.8, // Mais varia√ß√£o
    maxTokens: 100    // Resposta curta
  });

  const promptTemplate = ChatPromptTemplate.fromMessages([
    ['system', `Voc√™ √© a Marina do Saraiva Pets.

REGRAS IMPORTANTES:
- NUNCA use emojis
- Seja natural e casual (oi, vc, tb, pq)
- Resposta CURTA (m√°ximo 2 linhas)
- V√° direto ao ponto

Cliente disse: "{message}"

Responda de forma natural e convide para conversar sobre o pet.`],
    ['human', '{message}']
  ]);

  return RunnableSequence.from([
    // 1. Passa input direto
    RunnablePassthrough.assign({
      startTime: () => Date.now()
    }),

    // 2. Gera resposta com LLM
    RunnablePassthrough.assign({
      response: promptTemplate.pipe(llm).pipe(new StringOutputParser())
    }),

    // 3. Formata output
    async (state: any) => {
      const response = state.response as string;
      const processingTime = Date.now() - state.startTime;

      return {
        response,
        metadata: {
          pipelineUsed: 'SIMPLES',
          processingTime,
          characterCount: response.length
        }
      };
    }
  ]);
}

/**
 * üí∞ PIPELINE 2: CONVERS√ÉO
 * Para: Cliente demonstrando interesse em compra/agendamento
 * Objetivo: FECHAR VENDA
 */
export function createConversionPipeline(
  openaiApiKey: string,
  memory: StyleAwareMemory
) {
  const llm = new ChatOpenAI({
    openAIApiKey: openaiApiKey,
    modelName: 'gpt-4o-mini',
    temperature: 0.7,
    maxTokens: 150
  });

  const promptTemplate = ChatPromptTemplate.fromMessages([
    ['system', `Voc√™ √© a Marina, VENDEDORA do Saraiva Pets.

MODO CONVERS√ÉO ATIVO:
- Cliente est√° QUENTE (score: {conversionScore})
- FOCO: Fechar agendamento/venda AGORA
- Use t√©cnicas de fechamento assumido
- Ofere√ßa hor√°rios espec√≠ficos
- Crie senso de urg√™ncia/escassez

REGRAS:
- NUNCA use emojis
- Seja casual mas DIRETA
- M√°ximo 3 linhas
- SEMPRE termine com escolha bin√°ria

EXEMPLO:
"tenho vaga hj as 15h ou amanha as 10h
qual prefere?"

Cliente: "{message}"
{petName ? 'Pet: ' + petName : ''}

FECHE AGORA:`],
    ['human', '{message}']
  ]);

  return RunnableSequence.from([
    RunnablePassthrough.assign({
      startTime: () => Date.now()
    }),

    // Injeta dados de convers√£o no prompt
    RunnablePassthrough.assign({
      conversionScore: (input: any) => input.conversionScore || 80,
      petName: (input: any) => input.petName || ''
    }),

    // Gera resposta focada em convers√£o
    RunnablePassthrough.assign({
      response: promptTemplate.pipe(llm).pipe(new StringOutputParser())
    }),

    async (state: any) => ({
      response: state.response,
      metadata: {
        pipelineUsed: 'CONVERS√ÉO',
        processingTime: Date.now() - state.startTime,
        characterCount: state.response.length
      }
    })
  ]);
}

/**
 * ‚≠ê PIPELINE 3: VIP
 * Para: Clientes premium, alto valor
 * Objetivo: Tratamento EXCLUSIVO
 */
export function createVipPipeline(
  openaiApiKey: string,
  memory: StyleAwareMemory
) {
  const llm = new ChatOpenAI({
    openAIApiKey: openaiApiKey,
    modelName: 'gpt-4o-mini',
    temperature: 0.6,
    maxTokens: 150
  });

  const promptTemplate = ChatPromptTemplate.fromMessages([
    ['system', `Voc√™ √© a Marina, atendimento VIP do Saraiva Pets.

CLIENTE VIP DETECTADO:
- Tratamento PREMIUM
- Efici√™ncia (n√£o enrole)
- SEMPRE ofere√ßa a melhor op√ß√£o primeiro
- Use vocabul√°rio exclusivo: "premium", "exclusivo", "prioridade"

REGRAS:
- NUNCA use emojis
- Seja profissional mas casual
- M√°ximo 3 linhas
- Ofere√ßa hor√°rios VIP

Cliente VIP: "{userName}"
Pet: "{petName}"
Mensagem: "{message}"

Responda com excel√™ncia:`],
    ['human', '{message}']
  ]);

  return RunnableSequence.from([
    RunnablePassthrough.assign({
      startTime: () => Date.now(),
      userName: (input: any) => input.userName || 'cliente',
      petName: (input: any) => input.petName || 'seu pet'
    }),

    RunnablePassthrough.assign({
      response: promptTemplate.pipe(llm).pipe(new StringOutputParser())
    }),

    async (state: any) => ({
      response: state.response,
      metadata: {
        pipelineUsed: 'VIP',
        processingTime: Date.now() - state.startTime,
        characterCount: state.response.length
      }
    })
  ]);
}

/**
 * üß† PIPELINE 4: COMPLETO
 * Para: An√°lise comportamental completa
 * Objetivo: Resposta PERSONALIZADA com arqu√©tipo + RAG
 */
export function createCompletePipeline(
  openaiApiKey: string,
  memory: StyleAwareMemory,
  retrievalChain?: RetrievalChain
) {
  const llm = new ChatOpenAI({
    openAIApiKey: openaiApiKey,
    modelName: 'gpt-4o-mini',
    temperature: 0.8,
    maxTokens: 250
  });

  return RunnableSequence.from([
    RunnablePassthrough.assign({
      startTime: () => Date.now()
    }),

    // üîç RAG: Busca contexto relevante (se necess√°rio)
    RunnablePassthrough.assign({
      ragContext: async (input: any) => {
        // Verifica se deve usar RAG
        if (!retrievalChain || !RetrievalChain.shouldUseRAG(input.message)) {
          return null;
        }

        try {
          console.log('üîç RAG: Buscando contexto relevante...');
          const result = await retrievalChain.query(input.message, {
            chatId: input.chatId
          });

          if (result.usedContext) {
            console.log(`‚úÖ RAG: ${result.sources.length} fontes encontradas`);
            return result.answer;
          }

          return null;
        } catch (error: any) {
          console.error('‚ùå RAG: Erro ao buscar contexto:', error.message);
          return null;
        }
      }
    }),

    // Constr√≥i prompt din√¢mico baseado em arqu√©tipo
    RunnablePassthrough.assign({
      marinaMode: (input: any) => {
        if (input.archetype) {
          return getMarinaMode(input.archetype);
        }
        return '';
      }
    }),

    // Cria prompt template din√¢mico (com RAG se dispon√≠vel)
    async (state: any) => {
      let systemPrompt = `Voc√™ √© a Marina do Saraiva Pets.

CONTEXTO COMPORTAMENTAL:
- Sentimento: ${state.sentiment || 'neutro'}
- Urg√™ncia: ${state.urgency || 'normal'}
${state.userName ? `- Cliente: ${state.userName}` : ''}
${state.petName ? `- Pet: ${state.petName}` : ''}
${state.isNewClient ? '- CLIENTE NOVO (seja acolhedora)' : ''}

${state.marinaMode}`;

      // Injeta contexto RAG se dispon√≠vel
      if (state.ragContext) {
        systemPrompt += `\n\nINFORMA√á√ïES DA BASE DE CONHECIMENTO:
${state.ragContext}

IMPORTANTE: Use as informa√ß√µes acima para responder com precis√£o. N√ÉO invente dados.`;
      }

      systemPrompt += `\n\nREGRAS CR√çTICAS:
- NUNCA use emojis
- Seja natural (oi, vc, tb, pq, ne)
- M√°ximo 4 linhas
- N√ÉO repita respostas anteriores (varie!)

${state.fullContext || ''}

Cliente: "${state.message}"

Responda seguindo o modo Marina ativo:`;

      const prompt = ChatPromptTemplate.fromMessages([
        ['system', systemPrompt],
        ['human', '{message}']
      ]);

      const response = await prompt
        .pipe(llm)
        .pipe(new StringOutputParser())
        .invoke({ message: state.message });

      return {
        ...state,
        response
      };
    },

    // Verifica similaridade com StyleMemory
    async (state: any) => {
      const similarityCheck = await memory.checkSimilarity(
        state.chatId,
        state.response
      );

      if (similarityCheck.isSimilar) {
        console.log(`‚ö†Ô∏è Resposta similar detectada! Regenerando...`);

        // REGENERA com constraint de varia√ß√£o
        const retryPrompt = ChatPromptTemplate.fromMessages([
          ['system', `ATEN√á√ÉO: Sua √∫ltima resposta foi muito similar a esta:
"${similarityCheck.similarTo}"

REGENERE com VARIA√á√ÉO TOTAL:
- Use palavras diferentes
- Mude estrutura da frase
- Seja criativo mas mantenha naturalidade

Cliente: "${state.message}"

NOVA resposta (DIFERENTE):`],
          ['human', '{message}']
        ]);

        const newResponse = await retryPrompt
          .pipe(llm)
          .pipe(new StringOutputParser())
          .invoke({ message: state.message });

        return {
          ...state,
          response: newResponse,
          regenerated: true
        };
      }

      return state;
    },

    // Output final
    async (state: any) => ({
      response: state.response,
      metadata: {
        pipelineUsed: 'COMPLETO',
        processingTime: Date.now() - state.startTime,
        characterCount: state.response.length,
        regenerated: state.regenerated || false
      }
    })
  ]);
}

/**
 * Factory: Cria todos os pipelines (com RAG opcional)
 */
export function createAllPipelines(
  openaiApiKey: string,
  memory: StyleAwareMemory,
  retrievalChain?: RetrievalChain
) {
  return {
    simple: createSimplePipeline(openaiApiKey, memory),
    conversion: createConversionPipeline(openaiApiKey, memory),
    vip: createVipPipeline(openaiApiKey, memory),
    complete: createCompletePipeline(openaiApiKey, memory, retrievalChain)
  };
}

export type MarinaPipelines = ReturnType<typeof createAllPipelines>;
